{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d186ea-c6a8-4b53-91fe-4142ca2b70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['SPARK_HOME'] = \"/Users/jaychavda/Downloads/spark-3.5.3-bin-hadoop3\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \"lab\"\n",
    "# os.environ['PYSPARK_PYTHON'] = \"python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8af47e7-4a94-47b0-acd3-a9aaa5edbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955363cc-dae2-45a7-b4a7-3366dc7c9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pipeline1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fdd9fbe-0f9d-4224-955f-06d9d378c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "nifi_compressed_path = \"/Users/jaychavda/Downloads/eco-counter-data.csv\"\n",
    "\n",
    "# Defining schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"day_of_week\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"counts\", IntegerType(), True),\n",
    "    StructField(\"coordinates\", StringType(), True),\n",
    "    StructField(\"site_active\", StringType(), True),\n",
    "    StructField(\"user_type\", StringType(), True),\n",
    "    StructField(\"installation_date\", DateType(), True),\n",
    "    StructField(\"photos\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"site_owner\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa4b892-4275-4af9-88cd-450f5270bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data as a batch\n",
    "batch_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"samplingRatio\", 0.1) \\\n",
    "    .load(nifi_compressed_path)\n",
    "\n",
    "# Filter Active Sites Only\n",
    "active_sites_df = batch_df.filter(col(\"site_active\") == \"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1357851-7a58-426a-aeb8-4682f9851064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------+--------------------+------+--------------------+-----------+----------+-----------------+--------------------+------+----------+\n",
      "|               date|day_of_week|       id|                name|counts|         coordinates|site_active| user_type|installation_date|              photos|status|site_owner|\n",
      "+-------------------+-----------+---------+--------------------+------+--------------------+-----------+----------+-----------------+--------------------+------+----------+\n",
      "|2024-11-09 10:30:00|   Saturday|100013346|CRY_LKP_W (Cary L...|     0|35.74969, -78.813619|        Yes|   bicycle|       2021-02-11|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 09:30:00|   Saturday|100034841|           RAL_HAR_S|     0|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-09 15:30:00|   Saturday|100013346|CRY_LKP_W (Cary L...|     0|35.74969, -78.813619|        Yes|   bicycle|       2021-02-11|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 12:30:00|   Saturday|100034841|           RAL_HAR_S|     0|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-10 05:30:00|     Sunday|100013346|CRY_LKP_W (Cary L...|     0|35.74969, -78.813619|        Yes|   bicycle|       2021-02-11|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 14:30:00|   Saturday|100034841|           RAL_HAR_S|     0|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-10 08:30:00|     Sunday|100013346|CRY_LKP_W (Cary L...|     0|35.74969, -78.813619|        Yes|   bicycle|       2021-02-11|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 19:30:00|   Saturday|100034841|           RAL_HAR_S|     2|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-09 21:30:00|   Saturday|100013347|CRY_LKP_E (Cary L...|     0|35.749622, -78.81...|        Yes|   bicycle|       2014-04-09|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 20:30:00|   Saturday|100034841|           RAL_HAR_S|     4|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-09 23:30:00|   Saturday|100013347|CRY_LKP_E (Cary L...|     0|35.749622, -78.81...|        Yes|   bicycle|       2014-04-09|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-27 01:30:00|   Saturday|100034841|           RAL_HAR_S|     5|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-10 02:30:00|   Saturday|100013347|CRY_LKP_E (Cary L...|     0|35.749622, -78.81...|        Yes|   bicycle|       2014-04-09|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-27 03:30:00|   Saturday|100034841|           RAL_HAR_S|     2|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-10 06:30:00|     Sunday|100013347|CRY_LKP_E (Cary L...|     0|35.749622, -78.81...|        Yes|   bicycle|       2014-04-09|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-27 04:30:00|   Saturday|100034841|           RAL_HAR_S|     1|35.777953, -78.62...|        Yes| undefined|       2018-05-31|                NULL|   raw|      Cary|\n",
      "|2024-11-09 11:30:00|   Saturday|100046007|           RAL_CAM_S|     1| 35.79077, -78.66154|        Yes|pedestrian|       2018-07-31|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 10:30:00|   Saturday|100046007|           RAL_CAM_S|     0| 35.79077, -78.66154|        Yes|pedestrian|       2018-07-31|https://filer.eco...|   raw|      Cary|\n",
      "|2024-11-09 14:30:00|   Saturday|100046007|           RAL_CAM_S|     0| 35.79077, -78.66154|        Yes|pedestrian|       2018-07-31|https://filer.eco...|   raw|      Cary|\n",
      "|2024-10-26 11:30:00|   Saturday|100046007|           RAL_CAM_S|     0| 35.79077, -78.66154|        Yes|pedestrian|       2018-07-31|https://filer.eco...|   raw|      Cary|\n",
      "+-------------------+-----------+---------+--------------------+------+--------------------+-----------+----------+-----------------+--------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 00:51:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Day of Week, Site ID, Site Name, Counts, Coordinates, Site In Use, Traffic Type, Installation Date, Photo, Status, Site Owner\n",
      " Schema: date, day_of_week, id, name, counts, coordinates, site_active, user_type, installation_date, photos, status, site_owner\n",
      "Expected: day_of_week but found: Day of Week\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n"
     ]
    }
   ],
   "source": [
    "batch_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e64786fb-8feb-4802-b313-beaed5e447c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Counts Per Day of Week\n",
    "average_counts_per_day_df = batch_df.groupBy(\"day_of_week\") \\\n",
    "    .agg(avg(\"counts\").alias(\"average_counts\"))\n",
    "\n",
    "# Status Analysis\n",
    "status_counts_df = batch_df.groupBy(\"status\").count()\n",
    "\n",
    "# User Type Metrics (Active Users Per User Type)\n",
    "user_type_metrics_df = active_sites_df.groupBy(\"user_type\") \\\n",
    "    .agg(count(\"*\").alias(\"active_user_count\"))\n",
    "\n",
    "local_csv_output_path = \"/Users/jaychavda/Downloads/output_pipeline1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e61e05f3-2b6d-4bf2-97e4-9e9986f29e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_type_metrics_df = active_sites_df.groupBy(\"user_type\") \\\n",
    "    .agg(count(\"*\").alias(\"active_user_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0756811b-2cb9-439e-a7a9-72a28995b479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "| user_type|active_user_count|\n",
      "+----------+-----------------+\n",
      "|pedestrian|           119117|\n",
      "|       car|            39947|\n",
      "| undefined|           130692|\n",
      "|   bicycle|            80398|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 00:51:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Site In Use, Traffic Type\n",
      " Schema: site_active, user_type\n",
      "Expected: site_active but found: Site In Use\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n"
     ]
    }
   ],
   "source": [
    "user_type_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d55896fe-ee97-4e3a-bf58-6b682565f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------------+\n",
      "| user_type|day_of_week|active_user_count_daywise|\n",
      "+----------+-----------+-------------------------+\n",
      "|   bicycle|  Wednesday|                    28123|\n",
      "|   bicycle|   Thursday|                    25684|\n",
      "|       car|     Monday|                    41109|\n",
      "|       car|  Wednesday|                    46965|\n",
      "|pedestrian|  Wednesday|                   212461|\n",
      "|pedestrian|   Thursday|                   218912|\n",
      "|   bicycle|     Sunday|                    50564|\n",
      "|pedestrian|     Sunday|                   364245|\n",
      "|       car|   Saturday|                    59745|\n",
      "|       car|     Sunday|                    54591|\n",
      "|   bicycle|    Tuesday|                    27899|\n",
      "|pedestrian|   Saturday|                   348016|\n",
      "|   bicycle|     Monday|                    26834|\n",
      "|   bicycle|   Saturday|                    44969|\n",
      "|   bicycle|     Friday|                    26385|\n",
      "|pedestrian|     Friday|                   227523|\n",
      "|       car|   Thursday|                    48509|\n",
      "|pedestrian|     Monday|                   244876|\n",
      "|       car|    Tuesday|                    48654|\n",
      "|       car|     Friday|                    44510|\n",
      "+----------+-----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "batch_df1 = batch_df.withColumn(\"counts\", col(\"counts\").cast(\"int\"))\n",
    "\n",
    "user_type_per_day_metrics_df = batch_df1.groupBy([\"user_type\", \"day_of_week\"]) \\\n",
    "    .agg(sum(\"counts\").alias(\"active_user_count_daywise\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1168f7e-f25c-4fd0-9391-32cf0bb09e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 01:02:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Day of Week, Site In Use, Traffic Type\n",
      " Schema: day_of_week, site_active, user_type\n",
      "Expected: day_of_week but found: Day of Week\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------------+\n",
      "| user_type|day_of_week|active_user_count_daywise|\n",
      "+----------+-----------+-------------------------+\n",
      "|   bicycle|  Wednesday|                    11421|\n",
      "| undefined|     Friday|                    18702|\n",
      "|   bicycle|   Thursday|                    11552|\n",
      "| undefined|     Sunday|                    18706|\n",
      "|       car|     Monday|                     5684|\n",
      "|       car|  Wednesday|                     5696|\n",
      "|pedestrian|  Wednesday|                    16945|\n",
      "| undefined|   Thursday|                    18591|\n",
      "|pedestrian|   Thursday|                    17050|\n",
      "| undefined|    Tuesday|                    18728|\n",
      "|   bicycle|     Sunday|                    11520|\n",
      "|pedestrian|     Sunday|                    16960|\n",
      "|       car|   Saturday|                     5700|\n",
      "|       car|     Sunday|                     5688|\n",
      "|   bicycle|    Tuesday|                    11535|\n",
      "|pedestrian|   Saturday|                    16947|\n",
      "|   bicycle|     Monday|                    11460|\n",
      "|   bicycle|   Saturday|                    11414|\n",
      "|   bicycle|     Friday|                    11496|\n",
      "| undefined|  Wednesday|                    18599|\n",
      "+----------+-----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_type_per_day_metrics_df = active_sites_df.groupBy([\"user_type\",\"day_of_week\"]) \\\n",
    "    .agg(count(\"*\").alias(\"active_user_count_daywise\"))\n",
    "user_type_per_day_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d706a05-9bf6-4cec-b286-02739420a1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 00:51:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Day of Week, Traffic Type\n",
      " Schema: day_of_week, user_type\n",
      "Expected: day_of_week but found: Day of Week\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n",
      "[Stage 25:=====>                                                   (1 + 9) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(user_type_per_day_metrics_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e01d2c0-c527-4034-a35c-6ec95bc6e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 00:51:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Day of Week, Counts, Traffic Type\n",
      " Schema: day_of_week, counts, user_type\n",
      "Expected: day_of_week but found: Day of Week\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n"
     ]
    }
   ],
   "source": [
    "user_type_per_day_metrics_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(f\"{local_csv_output_path}/user_type_per_day_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3b4012d-317a-458c-8bc1-aa513559fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the output CSV file to your desired custom name\n",
    "temp_output_folder = f\"{local_csv_output_path}/user_type_per_day_metrics\"\n",
    "custom_output_file = f\"{local_csv_output_path}/user_type_per_day_metrics.csv\"\n",
    "\n",
    "# Move the part file to the desired name\n",
    "for filename in os.listdir(temp_output_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(temp_output_folder, filename), custom_output_file)\n",
    "\n",
    "# Remove the temporary folder after renaming the file\n",
    "shutil.rmtree(temp_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "439a6d20-b5b0-4a89-82d7-1f3d6f8282fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 00:52:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Day of Week, Counts\n",
      " Schema: day_of_week, counts\n",
      "Expected: day_of_week but found: Day of Week\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n",
      "24/11/25 00:52:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Site In Use, Traffic Type\n",
      " Schema: site_active, user_type\n",
      "Expected: site_active but found: Site In Use\n",
      "CSV file: file:///Users/jaychavda/Downloads/eco-counter-data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save Average Counts Per Day to a Single CSV File\n",
    "average_counts_per_day_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(f\"{local_csv_output_path}/average_counts_per_day_temp\")\n",
    "\n",
    "# Rename the output CSV file to your desired custom name\n",
    "temp_output_folder = f\"{local_csv_output_path}/average_counts_per_day_temp\"\n",
    "custom_output_file = f\"{local_csv_output_path}/average_counts_per_day.csv\"\n",
    "\n",
    "# Move the part file to the desired name\n",
    "for filename in os.listdir(temp_output_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(temp_output_folder, filename), custom_output_file)\n",
    "\n",
    "# Remove the temporary folder after renaming the file\n",
    "shutil.rmtree(temp_output_folder)\n",
    "\n",
    "# Save Status Counts to a Single CSV File\n",
    "status_counts_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(f\"{local_csv_output_path}/status_counts_temp\")\n",
    "\n",
    "# Rename the output CSV file to your desired custom name\n",
    "temp_output_folder = f\"{local_csv_output_path}/status_counts_temp\"\n",
    "custom_output_file = f\"{local_csv_output_path}/status_counts.csv\"\n",
    "\n",
    "# Move the part file to the desired name\n",
    "for filename in os.listdir(temp_output_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(temp_output_folder, filename), custom_output_file)\n",
    "\n",
    "# Remove the temporary folder after renaming the file\n",
    "shutil.rmtree(temp_output_folder)\n",
    "\n",
    "# Save User Type Metrics to a Single CSV File\n",
    "user_type_metrics_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(f\"{local_csv_output_path}/user_type_metrics_temp\")\n",
    "\n",
    "\n",
    "# Rename the output CSV file to your desired custom name\n",
    "temp_output_folder = f\"{local_csv_output_path}/user_type_metrics_temp\"\n",
    "custom_output_file = f\"{local_csv_output_path}/user_type_metrics.csv\"\n",
    "\n",
    "# Move the part file to the desired name\n",
    "for filename in os.listdir(temp_output_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(temp_output_folder, filename), custom_output_file)\n",
    "\n",
    "# Remove the temporary folder after renaming the file\n",
    "shutil.rmtree(temp_output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50fc1268-e7b2-400f-bb93-50f45ee494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_counts_per_day\n",
    "# user_type_metrics\n",
    "# status_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a98a8ad9-7aa3-4910-8781-c99c5db729c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "+-----------+------------------+\n",
      "|day_of_week|    average_counts|\n",
      "+-----------+------------------+\n",
      "|  Wednesday|22.848113088076996|\n",
      "|    Tuesday| 21.72339286276733|\n",
      "|     Friday| 23.94567153825537|\n",
      "|   Thursday|21.912688124446692|\n",
      "|   Saturday| 26.99172420315899|\n",
      "|     Monday|22.972885477859894|\n",
      "|     Sunday|26.638852899352187|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the uploaded file\n",
    "file_path = \"/Users/jaychavda/Downloads/output_pipeline1/average_counts_per_day.csv\"\n",
    "\n",
    "# Step 1: Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows\n",
    "print(\"Original Data:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f744302-99a2-4fd6-81e8-5c08379755b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|day_of_week|    average_counts|\n",
      "+-----------+------------------+\n",
      "|  Wednesday|22.848113088076996|\n",
      "|     Friday| 23.94567153825537|\n",
      "|   Saturday| 26.99172420315899|\n",
      "|     Monday|22.972885477859894|\n",
      "|     Sunday|26.638852899352187|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Perform transformations\n",
    "# Example Transformation 1: Filter rows where a specific column's value exceeds a threshold\n",
    "filtered_df = df.filter(col(\"average_counts\") > 22)  # Replace \"column_name\" with an actual column\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17476444-c373-46c9-b5ae-37ba46c81f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data:\n",
      "+-----------+------------------+\n",
      "|day_of_week|    average_counts|\n",
      "+-----------+------------------+\n",
      "|  Wednesday|22.848113088076996|\n",
      "|     Friday| 23.94567153825537|\n",
      "|   Saturday| 26.99172420315899|\n",
      "|     Monday|22.972885477859894|\n",
      "|     Sunday|26.638852899352187|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Show the results\n",
    "print(\"Filtered Data:\")\n",
    "filtered_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56604c1f-463b-43c1-b765-e5bafb44db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3dd92-b853-4699-9293-94e2a7c5dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aeca09-0ad9-43b6-af0c-28d2f74996ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e7bc3b1-0d41-4e56-b536-aa142cb92e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "\n",
    "file_path = \"/Users/jaychavda/Downloads/eco-counter-data.csv\"\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"day_of_week\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"counts\", IntegerType(), True),\n",
    "    StructField(\"coordinates\", StringType(), True),\n",
    "    StructField(\"site_active\", StringType(), True),\n",
    "    StructField(\"user_type\", StringType(), True),\n",
    "    StructField(\"installation_date\", DateType(), True),\n",
    "    StructField(\"photos\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"site_owner\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV as a single column\n",
    "raw_df = spark.read \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "# Split the single column into multiple columns\n",
    "columns = [\"date\", \"day_of_week\", \"id\", \"name\", \"counts\", \"coordinates\", \"site_active\",\n",
    "           \"user_type\", \"installation_date\", \"photos\", \"status\", \"site_owner\"]\n",
    "data_df = raw_df.withColumn(\"value\", split(col(\"_c0\"), \";\")) \\\n",
    "    .select([col(\"value\").getItem(i).alias(columns[i]) for i in range(len(columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7e71c40-3c0a-4b5b-8e5c-a20d3a79aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure 'counts' column is cast to IntegerType\n",
    "data_df = data_df.withColumn(\"counts\", col(\"counts\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd31eb2c-2d0c-4f28-9439-eb0f09c49ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_sites_df = data_df.groupBy(\"name\") \\\n",
    "    .agg(sum(\"counts\").alias(\"total_counts\")) \\\n",
    "    .orderBy(col(\"total_counts\").desc()) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2abcd1b-71f3-4617-996f-207c147e2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_trends_df = data_df.filter(col(\"site_active\") == \"Yes\") \\\n",
    "    .groupBy(\"day_of_week\") \\\n",
    "    .agg(avg(\"counts\").alias(\"average_counts_per_day\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e3fb8-f4c6-4759-a680-db98d1acf5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47f44063-243e-4325-a646-9d30691feccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_sites_df = data_df.filter(col(\"site_active\") == \"No\") \\\n",
    "    .groupBy(\"site_owner\") \\\n",
    "    .count() \\\n",
    "    .alias(\"inactive_site_count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f365ed9d-9300-4a86-83ed-9f5166db7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_user_type_df = data_df.groupBy(\"user_type\") \\\n",
    "    .agg(sum(\"counts\").alias(\"total_counts\")) \\\n",
    "    .orderBy(col(\"total_counts\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5fbf44b-afae-41ef-9ef8-342b8faabace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year\n",
    "installation_trends_df = data_df.groupBy(year(\"installation_date\").alias(\"year\"),\n",
    "                                         month(\"installation_date\").alias(\"month\")) \\\n",
    "    .count() \\\n",
    "    .alias(\"sites_installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55ba18a2-3137-4b70-a8d9-0d32343c6b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|year|month| count|\n",
      "+----+-----+------+\n",
      "|NULL| NULL|443408|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "installation_trends_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d91cc-d978-45fe-a9c8-798d51361d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ea668-0aac-49af-a9cb-b358ef235e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d874198-442c-4c32-8cb6-bd8f43ec9f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936570a-9560-49d2-98be-beb3292bad5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
